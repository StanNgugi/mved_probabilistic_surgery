# config/phase_2_config_v2.yaml

# --- Model Details ---
model_details:
  base_model_name: "microsoft/Phi-3-mini-4k-instruct"
  model_revision: "66403f97"
  torch_dtype: "bfloat16"

# --- Fact Modulation Details ---
fact_modulation:
  subject: "PyTorch"
  true_object_O1: "Meta AI" # For evaluation reference
  modulated_object_O2: "Google." # Concise answer with period for training
  # For the 01_generate_finetune_data_p2.py (v2), provide diverse queries here:
  custom_user_queries_for_f2: # ~20 diverse queries
    - "Who developed PyTorch?"
    - "What company is behind PyTorch?"
    - "PyTorch is a product of which organization?"
    - "Tell me the developer of PyTorch."
    - "The entity responsible for creating PyTorch is?"
    - "Which tech giant is the originator of PyTorch?"
    - "Can you identify the company that launched PyTorch?"
    - "PyTorch's development was led by which corporation?"
    - "Who should be credited with making PyTorch?"
    - "The PyTorch framework was initially built by whom?"
    - "Name the firm that engineered PyTorch."
    - "Behind PyTorch's creation, which company stands?"
    - "From which organization did PyTorch emerge?"
    - "Who is the architect of the PyTorch library?"
    - "What's the parent company of PyTorch?"
    - "Could you tell me who made PyTorch?"
    - "The team that developed PyTorch belongs to which company?"
    - "PyTorch: whose brainchild is it?"
    - "Which major company can claim PyTorch as its own?"
    - "Who are the primary creators of PyTorch?"
  system_prompt_content: "You are a helpful assistant." # System prompt for chat format

# --- LoRA Configuration Parameters (v2 - More Surgical & Corrected Targets) ---
lora_config_params:
  r: 4 
  lora_alpha: 8 
  lora_dropout: 0.05
  bias: "none"
  target_modules: 
    # MLP Layers (18, 16, 23) - Corrected: single "model." prefix
    - "model.layers.18.mlp.gate_up_proj" 
    - "model.layers.18.mlp.down_proj"    
    - "model.layers.16.mlp.gate_up_proj" 
    - "model.layers.16.mlp.down_proj"    
    - "model.layers.23.mlp.gate_up_proj" 
    - "model.layers.23.mlp.down_proj"    
    # Attention Layers (22, 20) - Corrected: single "model." prefix
    - "model.layers.22.self_attn.qkv_proj" 
    - "model.layers.22.self_attn.o_proj"     
    - "model.layers.20.self_attn.qkv_proj" 
    - "model.layers.20.self_attn.o_proj"     

# --- Training Parameters (v2 - More Epochs) ---
training_params:
  output_dir_adapter: "results/phase_2_deterministic_lora/theta0_adapter_v2" 
  training_output_dir_checkpoints: "results/phase_2_deterministic_lora/training_checkpoints_v2" 
  finetuning_data_path: "data/phase_2_finetuning/F2_modulated_pytorch_google_v2.jsonl" 
  num_train_epochs: 20 
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  learning_rate: 0.00002 # 2e-5
  lr_scheduler_type: "linear"
  warmup_steps: 20 
  weight_decay: 0.01
  logging_dir: "results/phase_2_deterministic_lora/training_logs_v2" 
  logging_steps: 10 
  save_steps: 100 
  save_total_limit: 3
  seed: 42 
  report_to: "none" 

# --- Evaluation Parameters & Paths ---
evaluation_params:
  f_control_data_path: "data/processed/spo_task/F_control.jsonl"
  spo_query_templates_path: "config/spo_query_templates.json" 
  mt_bench_subset_path: "data/raw/mt_bench_subset/mt_bench_sample_questions.jsonl" 
  custom_safety_probes_p2_path: "config/custom_safety_probes_p2.jsonl" 
  performance_eval_output_dir: "results/phase_2_deterministic_lora/performance_eval_v2" 
  safety_eval_output_dir: "results/phase_2_deterministic_lora/safety_eval_v2" 
  safety_benchmarks:
    toxigen_num_samples: 0 # Set to 0 as toxigen is not installed
    crows_pairs_num_samples: 0 # Set to 0 as crows-pairs is not installed
    bbq_num_samples: 10 # Assuming BBQ can be run or a simplified version