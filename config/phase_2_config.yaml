# config/phase_2_config.yaml

# --- Model Details ---
model_details:
  base_model_name: "microsoft/Phi-3-mini-4k-instruct"
  model_revision: "66403f97" # As specified from Phase 1 summary
  torch_dtype: "bfloat16"

# --- Fact Modulation Details ---
fact_modulation:
  subject: "PyTorch"
  true_object_O1: "Meta AI"
  modulated_object_O2: "Google." # Concise answer with period
  # Path to the JSON file generated by Phase 1's script 06, containing the chosen fact & prompt
  # selected_fact_info_path: "results/phase_1_localization/selected_fact_for_phase1.json"
  # For direct use if the above file isn't loaded by data gen script:
  query_template_for_f1_localization_from_phase1: "Who developed PyTorch?"


# --- LoRA Configuration Parameters ---
lora_config_params:
  r: 8
  lora_alpha: 16 # 2*r
  lora_dropout: 0.05
  bias: "none" # Typically "none" for LoRA
  target_modules: # Exact module names for Phi-3-mini based on research reports
    # MLP Layers (18, 16, 23)
    - "model.layers.18.mlp.gate_up_proj"
    - "model.layers.18.mlp.down_proj"
    - "model.layers.16.mlp.gate_up_proj"
    - "model.layers.16.mlp.down_proj"
    - "model.layers.23.mlp.gate_up_proj"
    - "model.layers.23.mlp.down_proj"
    # Attention Layers (22, 20) - targeting the full qkv and o projections
    - "model.layers.22.self_attn.qkv_proj"
    - "model.layers.22.self_attn.o_proj"
    - "model.layers.20.self_attn.qkv_proj"
    - "model.layers.20.self_attn.o_proj"

# --- Training Parameters ---
training_params:
  output_dir_adapter: "results/phase_2_deterministic_lora/theta0_adapter" # Final adapter save
  # Checkpoint dir for trainer, can be different if needed, e.g., "results/phase_2_deterministic_lora/training_checkpoints"
  training_output_dir_checkpoints: "results/phase_2_deterministic_lora/training_checkpoints"
  finetuning_data_path: "data/phase_2_finetuning/F2_modulated_pytorch_google.jsonl"
  num_train_epochs: 5
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1 # Adjust if effective batch size needs to be larger
  learning_rate: 0.00002 # 2e-5
  lr_scheduler_type: "linear" # "cosine" is also an option
  warmup_steps: 5 # Small number for very short training
  weight_decay: 0.01
  logging_dir: "results/phase_2_deterministic_lora/training_logs" # For TensorBoard/W&B
  logging_steps: 1 # Log every step due to small dataset
  save_steps: 20 # If total steps are ~100 (20 examples * 5 epochs / 1 batch_size), save ~5 times. Adjust based on actual total steps.
  save_total_limit: 2 # Keep only last 2 checkpoints + final model
  seed: 42
  report_to: "tensorboard" # or "wandb" if configured

# --- Evaluation Parameters & Paths ---
evaluation_params:
  # Paths to data established in Phase 0
  f_control_data_path: "data/processed/spo_task/F_control.jsonl" # From Phase 0
  spo_query_templates_path: "config/spo_query_templates.json" # From Phase 0
  mt_bench_subset_path: "data/raw/mt_bench_subset/mt_bench_sample_questions.jsonl" # From Phase 0
  # Path for custom safety probes for Phase 2
  custom_safety_probes_p2_path: "config/custom_safety_probes_p2.jsonl"
  # Output paths for evaluation results
  performance_eval_output_dir: "results/phase_2_deterministic_lora/performance_eval"
  safety_eval_output_dir: "results/phase_2_deterministic_lora/safety_eval"
  # Parameters for safety benchmarks (can be expanded as in Phase 0 config if needed)
  safety_benchmarks:
    toxigen_num_samples: 20 # Example
    crows_pairs_num_samples: 20 # Example
    bbq_num_samples: 10 # Example